{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NguyenDung278/CS114/blob/main/yolov5_Ageratum_conyzoides_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0iWfC_8cEOW",
        "outputId": "9ebc7367-6620-42bb-b943-0e8b791dd254"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fy8lBiQ13KHz"
      },
      "source": [
        "#Train"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "pLzKbqMx_DP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdxOTOGn2WAo",
        "outputId": "7d4ed119-91d1-4d3e-9c13-d76b23c39cd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 🚀 v7.0-116-g5c91dae Python-3.8.10 torch-1.13.1+cu116 CUDA:0 (Tesla T4, 15102MiB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete ✅ (2 CPUs, 12.7 GB RAM, 25.6/78.2 GB disk)\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/CS114\n",
        "!git clone https://github.com/ultralytics/yolov5\n",
        "%cd yolov5\n",
        "!pip install -qr requirements.txt\n",
        "import utils\n",
        "display = utils.notebook_init()  # checks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "id": "BoFXch5Mr6W6",
        "outputId": "2b76a7d9-cccc-420b-b0d7-746e7dc1c7da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 KB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m API key must be 40 characters long, yours was 64\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# Weights & Biases  (optional)\n",
        "%pip install -q wandb\n",
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfJPRQQrd12O",
        "outputId": "978c7421-a82d-491c-bb1c-2e1643ebbb07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: WARNING ⚠️ wandb is deprecated and will be removed in a future release. See supported integrations at https://github.com/ultralytics/yolov5#integrations.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m20520821\u001b[0m (\u001b[33mcs114_\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5m.pt, cfg=, data=/content/drive/MyDrive/CS114/dataset/dataset.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=12, batch_size=4, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ✅\n",
            "YOLOv5 🚀 v7.0-116-g5c91dae Python-3.8.10 torch-1.13.1+cu116 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n",
            "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "2023-03-03 05:39:07.127287: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-03-03 05:39:08.615657: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.8/dist-packages/cv2/../../lib64:/usr/lib64-nvidia\n",
            "2023-03-03 05:39:08.615793: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.8/dist-packages/cv2/../../lib64:/usr/lib64-nvidia\n",
            "2023-03-03 05:39:08.615814: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/.shortcut-targets-by-id/1vt4qnPC6V5SO0w69_hQBAGyvnGBASBdE/CS114/yolov5/wandb/run-20230303_053909-l47rxgua\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msolar-serenity-24\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cs114_/YOLOv5\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/cs114_/YOLOv5/runs/l47rxgua\u001b[0m\n",
            "Overriding model.yaml nc=80 with nc=1\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      5280  models.common.Conv                      [3, 48, 6, 2, 2]              \n",
            "  1                -1  1     41664  models.common.Conv                      [48, 96, 3, 2]                \n",
            "  2                -1  2     65280  models.common.C3                        [96, 96, 2]                   \n",
            "  3                -1  1    166272  models.common.Conv                      [96, 192, 3, 2]               \n",
            "  4                -1  4    444672  models.common.C3                        [192, 192, 4]                 \n",
            "  5                -1  1    664320  models.common.Conv                      [192, 384, 3, 2]              \n",
            "  6                -1  6   2512896  models.common.C3                        [384, 384, 6]                 \n",
            "  7                -1  1   2655744  models.common.Conv                      [384, 768, 3, 2]              \n",
            "  8                -1  2   4134912  models.common.C3                        [768, 768, 2]                 \n",
            "  9                -1  1   1476864  models.common.SPPF                      [768, 768, 5]                 \n",
            " 10                -1  1    295680  models.common.Conv                      [768, 384, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  2   1182720  models.common.C3                        [768, 384, 2, False]          \n",
            " 14                -1  1     74112  models.common.Conv                      [384, 192, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  2    296448  models.common.C3                        [384, 192, 2, False]          \n",
            " 18                -1  1    332160  models.common.Conv                      [192, 192, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  2   1035264  models.common.C3                        [384, 384, 2, False]          \n",
            " 21                -1  1   1327872  models.common.Conv                      [384, 384, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  2   4134912  models.common.C3                        [768, 768, 2, False]          \n",
            " 24      [17, 20, 23]  1     24246  models.yolo.Detect                      [1, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [192, 384, 768]]\n",
            "Model summary: 291 layers, 20871318 parameters, 20871318 gradients, 48.2 GFLOPs\n",
            "\n",
            "Transferred 475/481 items from yolov5m.pt\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 79 weight(decay=0.0), 82 weight(decay=0.0005), 82 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/.shortcut-targets-by-id/1vt4qnPC6V5SO0w69_hQBAGyvnGBASBdE/CS114/dataset/labels/train... 917 images, 0 backgrounds, 0 corrupt: 100% 917/917 [03:11<00:00,  4.78it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/drive/.shortcut-targets-by-id/1vt4qnPC6V5SO0w69_hQBAGyvnGBASBdE/CS114/dataset/labels/train.cache\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.8GB ram): 100% 917/917 [04:15<00:00,  3.58it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/.shortcut-targets-by-id/1vt4qnPC6V5SO0w69_hQBAGyvnGBASBdE/CS114/dataset/labels/val... 77 images, 0 backgrounds, 0 corrupt: 100% 77/77 [02:05<00:00,  1.63s/it]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/drive/.shortcut-targets-by-id/1vt4qnPC6V5SO0w69_hQBAGyvnGBASBdE/CS114/dataset/labels/val.cache\n",
            "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.1GB ram): 100% 77/77 [00:50<00:00,  1.52it/s]\n",
            "\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m2.91 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset ✅\n",
            "Plotting labels to runs/train/exp5/labels.jpg... \n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/train/exp5\u001b[0m\n",
            "Starting training for 12 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       0/11      1.71G    0.06363    0.03027          0          3        640: 100% 230/230 [00:45<00:00,  5.01it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 10/10 [00:02<00:00,  4.45it/s]\n",
            "                   all         77         88      0.488      0.455      0.441      0.115\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       1/11      1.94G    0.04771     0.0248          0          4        640: 100% 230/230 [00:40<00:00,  5.62it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 10/10 [00:00<00:00, 11.37it/s]\n",
            "                   all         77         88      0.616      0.639      0.612      0.256\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       2/11      1.94G    0.04192    0.02235          0          1        640: 100% 230/230 [00:39<00:00,  5.89it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 10/10 [00:01<00:00,  8.29it/s]\n",
            "                   all         77         88      0.576      0.784      0.685      0.291\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       3/11      1.94G    0.03592    0.02038          0          3        640: 100% 230/230 [00:41<00:00,  5.48it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 10/10 [00:00<00:00, 11.97it/s]\n",
            "                   all         77         88      0.948      0.824      0.876      0.431\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       4/11      1.95G    0.03335    0.01907          0          2        640: 100% 230/230 [00:41<00:00,  5.55it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 10/10 [00:00<00:00, 11.21it/s]\n",
            "                   all         77         88      0.764      0.734       0.73      0.341\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       5/11      1.95G    0.03038    0.01878          0          4        640: 100% 230/230 [00:39<00:00,  5.75it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 10/10 [00:01<00:00,  8.27it/s]\n",
            "                   all         77         88      0.879      0.825      0.849      0.402\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       6/11      1.95G    0.02896     0.0182          0          1        640: 100% 230/230 [00:38<00:00,  5.93it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 10/10 [00:00<00:00, 11.87it/s]\n",
            "                   all         77         88      0.882      0.807      0.815      0.384\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       7/11      1.95G    0.02652    0.01803          0          5        640: 100% 230/230 [00:40<00:00,  5.62it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 10/10 [00:00<00:00, 11.59it/s]\n",
            "                   all         77         88      0.904      0.795      0.794      0.392\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       8/11      1.95G     0.0235    0.01651          0          3        640: 100% 230/230 [00:41<00:00,  5.51it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 10/10 [00:01<00:00,  8.07it/s]\n",
            "                   all         77         88      0.873      0.784       0.81      0.391\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       9/11      1.95G    0.02371     0.0173          0          1        640: 100% 230/230 [00:39<00:00,  5.86it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 10/10 [00:00<00:00, 11.95it/s]\n",
            "                   all         77         88      0.907      0.777      0.852      0.384\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      10/11      1.95G    0.02197    0.01646          0          3        640: 100% 230/230 [00:40<00:00,  5.67it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 10/10 [00:00<00:00, 11.93it/s]\n",
            "                   all         77         88      0.975      0.807      0.871      0.389\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      11/11      1.95G     0.0216    0.01667          0          2        640: 100% 230/230 [00:38<00:00,  6.00it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 10/10 [00:01<00:00,  8.31it/s]\n",
            "                   all         77         88      0.913      0.784      0.806      0.401\n",
            "\n",
            "12 epochs completed in 0.144 hours.\n",
            "Optimizer stripped from runs/train/exp5/weights/last.pt, 42.2MB\n",
            "Optimizer stripped from runs/train/exp5/weights/best.pt, 42.2MB\n",
            "\n",
            "Validating runs/train/exp5/weights/best.pt...\n",
            "Fusing layers... \n",
            "Model summary: 212 layers, 20852934 parameters, 0 gradients, 47.9 GFLOPs\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 10/10 [00:02<00:00,  4.38it/s]\n",
            "                   all         77         88      0.948      0.824      0.877      0.432\n",
            "Results saved to \u001b[1mruns/train/exp5\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      metrics/mAP_0.5 ▁▄▅█▆█▇▇▇██▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: metrics/mAP_0.5:0.95 ▁▄▅█▆▇▇▇▇▇▇▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    metrics/precision ▁▃▂█▅▇▇▇▇▇█▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       metrics/recall ▁▄▇█▆██▇▇▇█▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       train/box_loss █▅▄▃▃▂▂▂▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       train/cls_loss ▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       train/obj_loss █▅▄▃▂▂▂▂▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val/box_loss █▆▄▄▂▂▂▂▂▁▁▁▄\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val/cls_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val/obj_loss █▅▂▁▃▂▃▄▅▄▅▆▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                x/lr0 █▅▂▂▂▂▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                x/lr1 ▃▆█▇▇▆▅▅▄▃▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                x/lr2 ▃▆█▇▇▆▅▅▄▃▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           best/epoch 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best/mAP_0.5 0.87584\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best/mAP_0.5:0.95 0.4314\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best/precision 0.94771\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          best/recall 0.82391\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      metrics/mAP_0.5 0.87657\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: metrics/mAP_0.5:0.95 0.43233\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    metrics/precision 0.94772\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       metrics/recall 0.82398\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       train/box_loss 0.0216\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       train/cls_loss 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       train/obj_loss 0.01667\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val/box_loss 0.02327\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val/cls_loss 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val/obj_loss 0.00949\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                x/lr0 0.00175\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                x/lr1 0.00175\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                x/lr2 0.00175\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33msolar-serenity-24\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cs114_/YOLOv5/runs/l47rxgua\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 17 media file(s), 3 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230303_053909-l47rxgua/logs\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: WARNING ⚠️ wandb is deprecated and will be removed in a future release. See supported integrations at https://github.com/ultralytics/yolov5#integrations.\n"
          ]
        }
      ],
      "source": [
        "!python train.py --img 640 --batch 4 --epochs 12 --data /content/drive/MyDrive/CS114/dataset/dataset.yaml --weights yolov5m.pt --cache "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OieSVOH63cFY"
      },
      "source": [
        "#Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKjt3zjRk8_L"
      },
      "outputs": [],
      "source": [
        "from utils.plots import plot_results \n",
        "plot_results('/content/drive/MyDrive/CS114/yolov5/runs/train/exp5/results.csv')  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LRL1yBnzlBr_"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TnTcrJ3llJB2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec277fbf-7512-46b4-9adb-01d3a6ca8af1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mdata=/content/drive/MyDrive/CS114/data_test/data_test.yaml, weights=['/content/drive/MyDrive/CS114/yolov5/runs/train/exp5/weights/last.pt'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.6, max_det=300, task=test, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=runs/val, name=exp, exist_ok=False, half=True, dnn=False\n",
            "YOLOv5 🚀 v7.0-116-g5c91dae Python-3.8.10 torch-1.13.1+cu116 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 212 layers, 20852934 parameters, 0 gradients, 47.9 GFLOPs\n",
            "\u001b[34m\u001b[1mtest: \u001b[0mScanning /content/drive/.shortcut-targets-by-id/1vt4qnPC6V5SO0w69_hQBAGyvnGBASBdE/CS114/data_test/labels... 108 images, 0 backgrounds, 0 corrupt: 100% 108/108 [00:31<00:00,  3.48it/s]\n",
            "\u001b[34m\u001b[1mtest: \u001b[0mNew cache created: /content/drive/.shortcut-targets-by-id/1vt4qnPC6V5SO0w69_hQBAGyvnGBASBdE/CS114/data_test/labels.cache\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:40<00:00, 10.11s/it]\n",
            "                   all        108        108          1          1      0.995      0.827\n",
            "Speed: 0.2ms pre-process, 14.3ms inference, 3.7ms NMS per image at shape (32, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/val/exp\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python val.py --weights /content/drive/MyDrive/CS114/yolov5/runs/train/exp5/weights/last.pt --data /content/drive/MyDrive/CS114/data_test/data_test.yaml --img 640 --iou 0.6 --half --task test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XrYlrFQ3i9g"
      },
      "source": [
        "#Try running the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ff98zPvy19zP"
      },
      "outputs": [],
      "source": [
        "!python detect.py --weights runs/train/exp5/weights/last.pt --img 640 --conf 0.45 --source /content/drive/MyDrive/CS114/videos/vidtest_1.mp4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YCQLDWMQ6JtH"
      },
      "outputs": [],
      "source": [
        "!python detect.py --weights runs/train/exp5/weights/last.pt --img 640 --conf 0.45 --source /content/drive/MyDrive/CS114/videos/vidtest_2.mp4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FTBIFWgmzU_W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d90fc40-246f-44fc-ab7e-1cec8559f7df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['runs/train/exp/weights/last.pt'], source=/content/drive/MyDrive/CS114/test2.jpg, data=data/coco128.yaml, imgsz=[640, 640], conf_thres=0.45, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
            "YOLOv5 🚀 v7.0-116-g5c91dae Python-3.8.10 torch-1.13.1+cu116 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 212 layers, 20852934 parameters, 0 gradients, 47.9 GFLOPs\n",
            "image 1/1 /content/drive/.shortcut-targets-by-id/1vt4qnPC6V5SO0w69_hQBAGyvnGBASBdE/CS114/test2.jpg: 576x640 1 cay_cut_lon, 26.8ms\n",
            "Speed: 0.8ms pre-process, 26.8ms inference, 1.7ms NMS per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/detect/exp3\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python detect.py --weights runs/train/exp5/weights/last.pt --img 640 --conf 0.45 --source /content/drive/MyDrive/CS114/test2.jpg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPlZb4d3zZO4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0de85773-f032-4b6c-9126-361f5fc9a861"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['runs/train/exp/weights/last.pt'], source=/content/drive/MyDrive/CS114/test4.jpg, data=data/coco128.yaml, imgsz=[640, 640], conf_thres=0.45, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
            "YOLOv5 🚀 v7.0-116-g5c91dae Python-3.8.10 torch-1.13.1+cu116 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 212 layers, 20852934 parameters, 0 gradients, 47.9 GFLOPs\n",
            "image 1/1 /content/drive/.shortcut-targets-by-id/1vt4qnPC6V5SO0w69_hQBAGyvnGBASBdE/CS114/test4.jpg: 640x512 1 cay_cut_lon, 28.5ms\n",
            "Speed: 3.3ms pre-process, 28.5ms inference, 2.2ms NMS per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/detect/exp4\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python detect.py --weights runs/train/exp5/weights/last.pt --img 640 --conf 0.45 --source /content/drive/MyDrive/CS114/test4.jpg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "BeE1EWjgzZnp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72c29a3f-f9a8-4f0b-ebdd-6292c4c1846f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['runs/train/exp5/weights/last.pt'], source=/content/drive/MyDrive/CS114/data_test/images/IMG_1104.jpg, data=data/coco128.yaml, imgsz=[640, 640], conf_thres=0.45, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
            "YOLOv5 🚀 v7.0-116-g5c91dae Python-3.8.10 torch-1.13.1+cu116 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 212 layers, 20852934 parameters, 0 gradients, 47.9 GFLOPs\n",
            "image 1/1 /content/drive/.shortcut-targets-by-id/1vt4qnPC6V5SO0w69_hQBAGyvnGBASBdE/CS114/data_test/images/IMG_1104.jpg: 640x480 1 cay_cut_lon, 21.7ms\n",
            "Speed: 0.6ms pre-process, 21.7ms inference, 1.5ms NMS per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/detect/exp8\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python detect.py --weights runs/train/exp5/weights/last.pt --img 640 --conf 0.45 --source /content/drive/MyDrive/CS114/data_test/images/IMG_1104.jpg"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kV7tZuvi7thS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Math"
      ],
      "metadata": {
        "id": "674O-9ED-22f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " model = torch.hub.load('ultralytics/yolov5', 'custom', path='/content/drive/MyDrive/CS114/yolov5/runs/train/exp5/weights/best.pt')"
      ],
      "metadata": {
        "id": "5YXKVEVF--eY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "574b3f34-fbaa-410f-c734-6e38b68624f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to /root/.cache/torch/hub/master.zip\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m YOLOv5 requirement \"setuptools>=65.5.1\" not found, attempting AutoUpdate...\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: setuptools>=65.5.1 in /usr/local/lib/python3.8/dist-packages (67.4.0)\n",
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m 1 package updated per /content/drive/.shortcut-targets-by-id/1vt4qnPC6V5SO0w69_hQBAGyvnGBASBdE/CS114/yolov5/requirements.txt\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m ⚠️ \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
            "\n",
            "YOLOv5 🚀 v7.0-116-g5c91dae Python-3.8.10 torch-1.13.1+cu116 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 212 layers, 20852934 parameters, 0 gradients, 47.9 GFLOPs\n",
            "Adding AutoShape... \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "imgs = ['/content/drive/MyDrive/CS114/data_test/images/IMG_1104.jpg']  # batch of images\n",
        "# Inference\n",
        "results = model(imgs)\n",
        "# Results\n",
        "results.print()\n",
        "results.save()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAYwnXTtCgrd",
        "outputId": "adba8a6f-6492-4e47-b251-6bc0c89980dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "image 1/1: 4032x3024 1 cay_cut_lon\n",
            "Speed: 282.1ms pre-process, 22.2ms inference, 2.1ms NMS per image at shape (1, 3, 640, 480)\n",
            "Saved 1 image to \u001b[1mruns/detect/exp6\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results.xyxy[0]  # predictions (tensor)\n",
        "results.pandas().xyxy[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "I1hUBukvLZo0",
        "outputId": "0a2016e7-a0b9-4d69-a810-8f5f0989780b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         xmin        ymin         xmax         ymax  confidence  class  \\\n",
              "0  230.309708  765.555115  2784.225586  2518.043945    0.532296      0   \n",
              "\n",
              "          name  \n",
              "0  cay_cut_lon  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b6ca33cc-f181-4fe0-ae3d-e0ce5572138e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>xmin</th>\n",
              "      <th>ymin</th>\n",
              "      <th>xmax</th>\n",
              "      <th>ymax</th>\n",
              "      <th>confidence</th>\n",
              "      <th>class</th>\n",
              "      <th>name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>230.309708</td>\n",
              "      <td>765.555115</td>\n",
              "      <td>2784.225586</td>\n",
              "      <td>2518.043945</td>\n",
              "      <td>0.532296</td>\n",
              "      <td>0</td>\n",
              "      <td>cay_cut_lon</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b6ca33cc-f181-4fe0-ae3d-e0ce5572138e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b6ca33cc-f181-4fe0-ae3d-e0ce5572138e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b6ca33cc-f181-4fe0-ae3d-e0ce5572138e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}